{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "from pathlib import Path\n",
    "from ast import literal_eval\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "sample_n = 100\n",
    "trial_n = 3 # Number of JSON parsing attempts\n",
    "output_step = 10 # Number of attempts for step 5\n",
    "\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA_TYPE = \"UQA\"\n",
    "# QA_TYPE = \"RQA\"\n",
    "QA_TYPE = \"AQuA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input CSV path\n",
    "path_csv = Path(f\"../data/input/{QA_TYPE}.csv\")\n",
    "path_csv.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_id = \"question_id\"\n",
    "# col_evidence = \"evidence_wo_url\"\n",
    "# col_evidence = \"evidence\"\n",
    "col_evidence = \"rationale\"\n",
    "col_question = \"question\"\n",
    "# col_question = \"question_sentence\"\n",
    "# col_choices = \"choices\"\n",
    "col_choices = \"options\"\n",
    "# col_answer = \"answer\"\n",
    "col_answer = \"correct\"\n",
    "\n",
    "list_convert_cols = [col_choices]\n",
    "# list_convert_cols = [col_choices, col_answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = \"gemma2:9b\"\n",
    "openai_model = \"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output paths\n",
    "dir_output = Path(f\"../data/output/{QA_TYPE}_case3_{openai_model}/\")\n",
    "if not dir_output.exists():\n",
    "    dir_output.mkdir()\n",
    "\n",
    "output_path_qa = dir_output / f\"{QA_TYPE}_{ollama_model.replace(':', '_')}_{{trial_no}}.csv\"\n",
    "output_path_qa_tmp = dir_output / f\"{QA_TYPE}_{ollama_model.replace(':', '_')}_tmp.csv\" # 途中経過\n",
    "output_path_step4 = dir_output / f\"{QA_TYPE}_step4.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the intermediate CSV file\n",
    "dir_input = Path(f\"../data/output/{QA_TYPE}_case3_gpt-4o-mini-2024-07-18/\")\n",
    "\n",
    "input_path_qa_tmp = dir_input / f\"{QA_TYPE}_{ollama_model.replace(':', '_')}_tmp.csv\" # 途中経過\n",
    "\n",
    "input_path_step1 = dir_input / f\"{QA_TYPE}_step1.csv\"\n",
    "input_path_step2 = dir_input / f\"{QA_TYPE}_step2.csv\"\n",
    "input_path_step2_use = dir_input / f\"{QA_TYPE}_step2_use.csv\"\n",
    "input_path_step4 = dir_input / f\"{QA_TYPE}_step4.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of parts of speech to be masked\n",
    "POS_CONTENT_WORD = ['PROPN', 'NOUN', 'VERB', 'ADJ', 'ADV', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mask_rate = list(range(0, 85, 20))\n",
    "print(list_mask_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP5_PROMPT = \"\"\"The following is a text and metadata related to the code terms within the text. Answer the question concisely according to the instructions.\n",
    "\n",
    "## Instructions\n",
    "- Choose the answer from the options and respond with the corresponding number.\n",
    "- Respond in JSON format as {{'basis': str, 'answer': int}}\n",
    "- Use only the text as a reference for the basis\n",
    "\n",
    "## Text\n",
    "{context}\n",
    "\n",
    "## Metadata\n",
    "{metadata}\n",
    "\n",
    "## Question\n",
    "{question}\n",
    "\n",
    "## Options\n",
    "{option_list}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_async_client = AsyncOpenAI()\n",
    "ollama_client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = 'user'\n",
    "AI = 'assistant'\n",
    "SYS = 'system'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(client:OpenAI, model:str, messages:list[dict], stream:bool=True, output_json:bool=False):\n",
    "    param = {\n",
    "        'model':model,\n",
    "        'messages':messages,\n",
    "        'stream':stream\n",
    "    }\n",
    "    \n",
    "    if output_json:\n",
    "        param['response_format'] = {'type': 'json_object'}\n",
    "\n",
    "    return client.chat.completions.create(**param)\n",
    "\n",
    "def write_stream(stream) -> str:\n",
    "    ret = \"\"\n",
    "    for c in stream:\n",
    "        dlt = c.choices[0].delta.content\n",
    "        if dlt:\n",
    "            ret += dlt\n",
    "            print(dlt, end=\"\", flush=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def append_message(role:str, content:str, messages:list[dict]=[]):\n",
    "    messages.append({'role': role, 'content': content})\n",
    "    return messages\n",
    "\n",
    "async def aget_response(client:AsyncOpenAI, model:str, messages:list[dict], output_json:bool=False) -> str:\n",
    "    param = {\n",
    "        'model':model,\n",
    "        'messages':messages\n",
    "    }\n",
    "    if output_json:\n",
    "        param['response_format'] = {'type': 'json_object'}\n",
    "\n",
    "    ret = await client.chat.completions.create(**param)\n",
    "    return ret.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_markdown(df:pd.DataFrame):\n",
    "    meta_table_s2 = ' | '.join(df.columns) + '\\n'\n",
    "    meta_table_s2 += ' | '.join(['---']*len(df.columns.to_list())) + \"\\n\"\n",
    "    meta_table_s2 += '\\n'.join(df.apply(lambda x: ' | '.join([x[col] for col in df.columns]), axis=1).values)\n",
    "\n",
    "    return meta_table_s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {   \n",
    "    'converters': {col:literal_eval for col in list_convert_cols + ['s3_output_choices']}\n",
    "}\n",
    "df_qa = pd.read_csv(input_path_qa_tmp, **param).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_new = pd.read_csv(input_path_step2_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. regular masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step4\n",
    "- Select words to be coded based on the masking rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_mask_row(df_word:pd.DataFrame, mask_rate:int):\n",
    "    \"\"\"Select words to be coded based on the masking rate\"\"\"\n",
    "    assert (mask_rate >= 0) and (mask_rate <= 100), f\"`mask_rate` must be set between 0 and 100. mask_rate: {mask_rate}\"\n",
    "\n",
    "    df_copy = df_word.copy()\n",
    "    mask_col = f'p_{mask_rate}_masked'\n",
    "    df_copy[mask_col] = False\n",
    "\n",
    "    for qa_id, rows in df_copy.groupby(col_id):\n",
    "        list_lemma = rows['lemma'].unique().tolist()\n",
    "        mask_row_n = round(len(list_lemma) * (mask_rate/100))\n",
    "        print(f'ID_{qa_id} {mask_rate}% number of words to be masked:', mask_row_n)\n",
    "\n",
    "        list_mask_lemma = random.sample(list_lemma, k=mask_row_n)\n",
    "        mask_rows_index = rows[rows['lemma'].isin(list_mask_lemma)].index.values\n",
    "\n",
    "        df_copy.loc[mask_rows_index, mask_col] = True\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mask_rate in list_mask_rate:\n",
    "    df_word_new = select_mask_row(df_word_new, mask_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step4(df_qa:pd.DataFrame, list_mask_rate:list[int], df_word_new:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add masked text based on the masking rate to df_qa\"\"\"\n",
    "    df_copy = df_qa.copy()\n",
    "    for mask_rate in list_mask_rate:\n",
    "        df_copy[f's4_prg_encode_context_{mask_rate}'] = \"\"\n",
    "        df_copy[f's4_prg_encode_Q_{mask_rate}'] = \"\"\n",
    "        # Use `s3_output_choices` for `Choices` only this time\n",
    "        df_copy[f's4_prg_encode_choices_{mask_rate}'] = df_copy['s3_output_choices']\n",
    "\n",
    "        for i, row in df_copy.iterrows():\n",
    "            qa_id = row[col_id]\n",
    "            values = df_word_new[(df_word_new[col_id]==qa_id) & (df_word_new[f'p_{mask_rate}_masked'])].apply(lambda x: {x['code']:x['word']}, axis=1)\n",
    "            values = sorted(values, key=lambda x: x[next(iter(x))].count(' '), reverse=True)\n",
    "            \n",
    "            # for col in ['s3_output_context', 's3_output_Q', 's3_output_choices']:\n",
    "            for col in ['s3_output_Q']:\n",
    "                sub_text = row[col]\n",
    "                for pair in values:\n",
    "                    code = list(pair.keys())[0]\n",
    "                    word = list(pair.values())[0]\n",
    "                    sub_text = sub_text.replace(word, f\"<{code}>\")\n",
    "                df_copy.loc[i, col.replace('s3_output_', 's4_prg_encode_')+f\"_{mask_rate}\"] = sub_text\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa = step4(df_qa[[c for c in df_qa.columns.values if not c.startswith('s4_')]], list_mask_rate, df_word_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa.to_csv(output_path_qa_tmp, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_new.to_csv(output_path_step4, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_new = pd.read_csv(output_path_step4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the missing rate of meanings for each masking rate\n",
    "list_tmp = []\n",
    "for mask_rate in list_mask_rate:\n",
    "    if mask_rate == 0:\n",
    "        continue\n",
    "    \n",
    "    tmp_total_lemma = df_word_new[df_word_new[f'p_{mask_rate}_masked']]['lemma'].nunique()\n",
    "    tmp_empty_meaning_lemma = tmp_total_lemma - df_word_new[df_word_new[f'p_{mask_rate}_masked'] & df_word_new['meaning'].notna()]['lemma'].nunique()\n",
    "    list_tmp.append({'MR': mask_rate, 'number of words with missing meanings': tmp_empty_meaning_lemma, 'word count': tmp_total_lemma, 'missing rate of meanings': tmp_empty_meaning_lemma / tmp_total_lemma})\n",
    "    \n",
    "pd.DataFrame(list_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step5\n",
    "- generate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def exec_step5(client:AsyncOpenAI, model:str, i:int, output_col:str, query_col:str, context:str, question:str, choices: list, metadata: str, count:int):\n",
    "    messages = []\n",
    "    prompt = STEP5_PROMPT.format(\n",
    "        context=context,\n",
    "        metadata=metadata,\n",
    "        question=question,\n",
    "        option_list=[f\"{i}. {val}\" for i, val in enumerate(choices, 1)]\n",
    "    )\n",
    "    messages = append_message(USER, prompt, messages)\n",
    "\n",
    "    while count > 0:\n",
    "        try:\n",
    "            res = await aget_response(client, model, messages, output_json=True)\n",
    "            ret = (i, json.loads(res)['answer'], prompt, output_col, query_col)\n",
    "            return ret\n",
    "\n",
    "        except Exception as e:\n",
    "            print('error', e)\n",
    "            count -= 1\n",
    "\n",
    "    return (i, '-1', '', output_col, query_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def step5(aclient:AsyncOpenAI, model:str, df_qa:pd.DataFrame, df_word_new:pd.DataFrame) -> pd.DataFrame:\n",
    "    df_copy = df_qa.copy()\n",
    "    print('model:', model)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for mask_rate in list_mask_rate:\n",
    "        tasks = []\n",
    "        \n",
    "        output_col = f'answer_{model}_{mask_rate}'\n",
    "        query_col = f'query_{mask_rate}'\n",
    "        df_copy[output_col] = \"\"\n",
    "        df_copy[query_col] = \"\"\n",
    "        \n",
    "        col_context = f's4_prg_encode_context_{mask_rate}'\n",
    "        col_Q = f's4_prg_encode_Q_{mask_rate}'\n",
    "        col_choices_tmp = f's4_prg_encode_choices_{mask_rate}'\n",
    "        \n",
    "        for i, row in df_copy.iterrows():\n",
    "            qa_id = row[col_id]\n",
    "            \n",
    "            meta_table = df_word_new[(df_word_new[col_id]==qa_id) & (df_word_new[f'p_{mask_rate}_masked'])].groupby('lemma').agg(\n",
    "                {col: 'unique' for col in ['part_of_speech', 'category', 'meaning', 'code']}\n",
    "            ).copy()\n",
    "            if not meta_table.empty:\n",
    "                for col in ['part_of_speech', 'category', 'meaning', 'code']:\n",
    "                    meta_table[col] = meta_table[col].apply(lambda x: ', '.join(t for t in x if type(t)==str))\n",
    "            md_metatable = convert_df_to_markdown(meta_table)\n",
    "            count = trial_n\n",
    "            tasks.append(asyncio.ensure_future(exec_step5(aclient, model, i, output_col, query_col, row[col_context], row[col_Q], row[col_choices_tmp], md_metatable, count)))\n",
    "\n",
    "        results.extend(await asyncio.gather(*tasks))\n",
    "\n",
    "    for i, ans, query, output_col, query_col in results:\n",
    "        df_copy.loc[i, output_col] = ans\n",
    "        df_copy.loc[i, query_col] = query\n",
    "\n",
    "    return df_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output_num in range(1, output_step+1):\n",
    "    print('trial:', output_num) \n",
    "    df_result = await step5(openai_async_client, openai_model, df_qa, df_word_new)\n",
    "    save_path_tmp = str(output_path_qa).format(trial_no=output_num)\n",
    "    print(save_path_tmp)\n",
    "    df_result.to_csv(save_path_tmp, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. partial lifting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {   \n",
    "    'converters': {col:literal_eval for col in list_convert_cols + ['s3_output_choices']}\n",
    "}\n",
    "df_qa = pd.read_csv(output_path_qa_tmp, **param).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_new_all_meaning = df_word_new[df_word_new['meaning'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa_all_meaning = step4(df_qa, list_mask_rate, df_word_new_all_meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tmp = []\n",
    "for mask_rate in list_mask_rate:\n",
    "    if mask_rate == 0:\n",
    "        continue\n",
    "\n",
    "    tmp_total_lemma = df_word_new_all_meaning[df_word_new_all_meaning[f'p_{mask_rate}_masked']]['lemma'].nunique()\n",
    "    tmp_empty_meaning_lemma = tmp_total_lemma - df_word_new_all_meaning[df_word_new_all_meaning[f'p_{mask_rate}_masked'] & df_word_new_all_meaning['meaning'].notna()]['lemma'].nunique()\n",
    "    list_tmp.append({'MR': mask_rate, 'number of words with missing meanings': tmp_empty_meaning_lemma, 'word count': tmp_total_lemma, 'missing rate of meanings': tmp_empty_meaning_lemma / tmp_total_lemma})\n",
    "    \n",
    "pd.DataFrame(list_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output_num in range(1, output_step+1):\n",
    "    print('trial:', output_num) \n",
    "    df_result = await step5(openai_async_client, openai_model, df_qa_all_meaning, df_word_new_all_meaning)\n",
    "    save_path_tmp = str(output_path_qa).format(trial_no=f\"{output_num}_filtered\")\n",
    "    print(save_path_tmp)\n",
    "    df_result.to_csv(save_path_tmp, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. strict masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {   \n",
    "    'converters': {col:literal_eval for col in list_convert_cols + ['s3_output_choices']}\n",
    "}\n",
    "df_qa_no_meaning = pd.read_csv(output_path_qa_tmp, **param).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_new_no_meaning = pd.read_csv(output_path_step4)\n",
    "df_word_new_no_meaning['meaning'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa_no_meaning = step4(df_qa_no_meaning, list_mask_rate, df_word_new_no_meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tmp = []\n",
    "for mask_rate in list_mask_rate:\n",
    "    if mask_rate == 0:\n",
    "        continue\n",
    "    \n",
    "    tmp_total_lemma = df_word_new_no_meaning[df_word_new_no_meaning[f'p_{mask_rate}_masked']]['lemma'].nunique()\n",
    "    tmp_empty_meaning_lemma = tmp_total_lemma - df_word_new_no_meaning[df_word_new_no_meaning[f'p_{mask_rate}_masked'] & df_word_new_no_meaning['meaning'].notna()]['lemma'].nunique()\n",
    "    list_tmp.append({'MR': mask_rate, 'number of words with missing meanings': tmp_empty_meaning_lemma, 'word count': tmp_total_lemma, 'missing rate of meanings': tmp_empty_meaning_lemma / tmp_total_lemma})\n",
    "    \n",
    "pd.DataFrame(list_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output_num in range(1, output_step+1):\n",
    "    print('trial:', output_num) \n",
    "    df_result = await step5(openai_async_client, openai_model, df_qa_no_meaning, df_word_new_no_meaning)\n",
    "    save_path_tmp = str(output_path_qa).format(trial_no=f\"{output_num}_no_meaning\")\n",
    "    print(save_path_tmp)\n",
    "    df_result.to_csv(save_path_tmp, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. lenient masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {   \n",
    "    'converters': {col:literal_eval for col in list_convert_cols + ['s3_output_choices']}\n",
    "}\n",
    "df_qa_no_verb = pd.read_csv(output_path_qa_tmp, **param).fillna(\"\")\n",
    "df_word_new_no_verb = pd.read_csv(output_path_step4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_lemma = df_word_new_no_verb.groupby([col_id, 'lemma'])['part_of_speech'].apply('unique').reset_index()\n",
    "unique_lemma['contains_verb'] = unique_lemma['part_of_speech'].apply(lambda x: 'verb' in [pos.lower() for pos in x if pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_new_no_verb = df_word_new_no_verb.merge(unique_lemma.rename(columns={'part_of_speech': 'POS_unique'}), how='left', on=[col_id, 'lemma'])\n",
    "df_word_new_no_verb.loc[df_word_new_no_verb['contains_verb'], [f'p_{mr}_masked' for mr in list_mask_rate]] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa_no_verb = step4(df_qa_no_verb, list_mask_rate, df_word_new_no_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tmp = []\n",
    "for mask_rate in list_mask_rate:\n",
    "    if mask_rate == 0:\n",
    "        continue\n",
    "    \n",
    "    tmp_total_lemma = df_word_new_no_verb[df_word_new_no_verb[f'p_{mask_rate}_masked']]['lemma'].nunique()\n",
    "    tmp_empty_meaning_lemma = tmp_total_lemma - df_word_new_no_verb[df_word_new_no_verb[f'p_{mask_rate}_masked'] & df_word_new_no_verb['meaning'].notna()]['lemma'].nunique()\n",
    "    list_tmp.append({'MR': mask_rate, 'number of words with missing meanings': tmp_empty_meaning_lemma, 'word count': tmp_total_lemma, 'missing rate of meanings': tmp_empty_meaning_lemma / tmp_total_lemma})\n",
    "    \n",
    "pd.DataFrame(list_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output_num in range(1, output_step+1):\n",
    "    print('trial:', output_num) \n",
    "    df_result = await step5(openai_async_client, openai_model, df_qa_no_verb, df_word_new_no_verb)\n",
    "    save_path_tmp = str(output_path_qa).format(trial_no=f\"{output_num}_no_verb\")\n",
    "    print(save_path_tmp)\n",
    "    df_result.to_csv(save_path_tmp, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
