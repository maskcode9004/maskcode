{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = \"gemma2:9b\"\n",
    "openai_model = \"gpt-4o-mini-2024-07-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_async_client = AsyncOpenAI()\n",
    "ollama_client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = 'user'\n",
    "AI = 'assistant'\n",
    "SYS = 'system'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(client:OpenAI, model:str, messages:list[dict], stream:bool=True, output_json:bool=False):\n",
    "    param = {\n",
    "        'model':model,\n",
    "        'messages':messages,\n",
    "        'stream':stream\n",
    "    }\n",
    "    \n",
    "    if output_json:\n",
    "        param['response_format'] = {'type': 'json_object'}\n",
    "\n",
    "    return client.chat.completions.create(**param)\n",
    "\n",
    "def write_stream(stream) -> str:\n",
    "    ret = \"\"\n",
    "    for c in stream:\n",
    "        dlt = c.choices[0].delta.content\n",
    "        if dlt:\n",
    "            ret += dlt\n",
    "            print(dlt, end=\"\", flush=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def append_message(role:str, content:str, messages:list[dict]=[]):\n",
    "    messages.append({'role': role, 'content': content})\n",
    "    return messages\n",
    "\n",
    "async def aget_response(client:AsyncOpenAI, model:str, messages:list[dict], output_json:bool=False) -> str:\n",
    "    param = {\n",
    "        'model':model,\n",
    "        'messages':messages\n",
    "    }\n",
    "    if output_json:\n",
    "        param['response_format'] = {'type': 'json_object'}\n",
    "\n",
    "    ret = await client.chat.completions.create(**param)\n",
    "    return ret.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"\"\"We will simulate the sales plan after the recall of the ZX-1000 model based on the following sales plan. Please fill in the blanks in the simulation according to the conditions.\n",
    "\n",
    "<Reference 3: Sales Plan>\n",
    "Scooter Model: ZX-1000\n",
    "2023 Production Volume: 15,840.00 units\n",
    "2024 Production Plan: 27,720.00 units\n",
    "Domestic Inventory as of the end of April 2024: 3,960.00 units\n",
    "\n",
    "Projected Revenue for This Fiscal Year: 2,772.00 million yen\n",
    "\n",
    "(ZX-1000 Domestic Projected Revenue: 1,980.00 million yen)\n",
    "</Reference 3: Sales Plan>\n",
    "\n",
    "#Conditions\n",
    "The recall cost per unit is set at 8,000 yen, which includes all costs such as parts, repairs, transportation, and other expenses.\n",
    "The post-recall sales volume N is estimated with a reduction rate of 25%.\n",
    "\n",
    "#Simulation\n",
    "Let A be the production volume in 2023, B the production plan volume for 2024, C the inventory volume as of April 2024, D the planned revenue for this fiscal year, and E the planned revenue for this fiscal year for the model subject to recall.\n",
    "The number of units sold subject to recall, NR: calculated by subtracting the number of units remaining unsold as of April 2024 from the 2023 production volume A, i.e., NR = A - C =\n",
    "The sales price of the model subject to recall, P: calculated by dividing the planned sales revenue E of the model by the total of the production plan volume B and the inventory volume C for 2024, i.e., P = E / (B + C) =\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mask_rate = [20, 40, 60, 80, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA_TYPE = \"UQA\"\n",
    "# QA_TYPE = \"RQA\"\n",
    "QA_TYPE = \"MskCal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output paths\n",
    "dir_output = Path(f\"../data/output/{QA_TYPE}_2/\")\n",
    "if not dir_output.exists():\n",
    "    dir_output.mkdir()\n",
    "\n",
    "output_path_qa = dir_output / f\"{QA_TYPE}_{ollama_model.replace(':', '_')}_{{trial_no}}.csv\"\n",
    "output_path_qa_tmp = dir_output / f\"{QA_TYPE}_{ollama_model.replace(':', '_')}_tmp.csv\" # 途中経過\n",
    "\n",
    "output_path_step1 = dir_output / f\"{QA_TYPE}_step1.csv\"\n",
    "output_path_step2 = dir_output / f\"{QA_TYPE}_step2.csv\"\n",
    "output_path_step2_use = dir_output / f\"{QA_TYPE}_step2_use.csv\"\n",
    "output_path_step4 = dir_output / f\"{QA_TYPE}_step4.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"merge_hyphenated\")\n",
    "def merge_hyphenated(doc):\n",
    "    \"\"\"Detect hyphenated words as a single word\"\"\"\n",
    "    spans = []\n",
    "    for i in range(len(doc) - 2):\n",
    "        if doc[i + 1].text == '-' and not doc[i + 1].whitespace_:\n",
    "            spans.append(doc[i:i+3])\n",
    "    filtered_spans = filter_spans(spans)\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in filtered_spans:\n",
    "            retokenizer.merge(span)\n",
    "    return doc\n",
    "\n",
    "def get_entity_token_index(doc:spacy.tokens.doc.Doc, start_i:int) -> list[int]:\n",
    "    \"\"\"Search until the end of the entity token and return the index up to the end position\"\"\"\n",
    "    list_ind = [start_i]\n",
    "    if len(doc) <= (start_i+1):\n",
    "        return list_ind\n",
    "    \n",
    "    cur = start_i + 1\n",
    "    while cur < len(doc):\n",
    "        if doc[cur].ent_iob_ == \"O\":\n",
    "            return list_ind\n",
    "        \n",
    "        list_ind.append(cur)\n",
    "        cur += 1\n",
    "\n",
    "    return list_ind\n",
    "\n",
    "def process(nlp:spacy.language.Language, text:str) -> pd.DataFrame:\n",
    "    \"\"\"Output a list of morphemes from the text and return a dataframe of words (including duplicates)\"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    list_words = []\n",
    "    list_checked_i = []\n",
    "\n",
    "    for token in doc:\n",
    "        # entity token\n",
    "        if token.ent_type_:\n",
    "            # Start token of the entity\n",
    "            if token.ent_iob_ == \"B\":\n",
    "                tmp_index = get_entity_token_index(doc, token.i)\n",
    "                tmp_words = doc[tmp_index[0]:tmp_index[-1]+1].text\n",
    "                tmp_lemma = doc[tmp_index[0]:tmp_index[-1]+1].lemma_\n",
    "                list_words.append(\n",
    "                    {\n",
    "                        'word': tmp_words,\n",
    "                        'part_of_speech': token.pos_,\n",
    "                        'category': token.ent_type_, \n",
    "                        'lemma': tmp_lemma,\n",
    "                        'word_count': len(tmp_words.split()),\n",
    "                        'index': tmp_index\n",
    "                    }\n",
    "                )\n",
    "                list_checked_i.extend(tmp_index)\n",
    "        else:\n",
    "            word_data = {\n",
    "                    'word': token.text,\n",
    "                    'part_of_speech': token.pos_,\n",
    "                    'category': token.ent_type_,\n",
    "                    'lemma': token.lemma_,\n",
    "                    'word_count': len(token.text.split()),\n",
    "                    'index':[token.i]\n",
    "                }\n",
    "            \n",
    "            if token.i in list_checked_i:\n",
    "                continue\n",
    "            \n",
    "            if token.pos_ in ['PUNCT', 'SPACE']:\n",
    "                word_data['word_count'] = 0\n",
    "\n",
    "            list_words.append(\n",
    "                word_data\n",
    "            )\n",
    "            list_checked_i.append(token.i)\n",
    "\n",
    "    df = pd.DataFrame(list_words)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1(list_text:list) -> pd.DataFrame:\n",
    "    \"\"\"Extract morphemes from the text column of the QA\"\"\"\n",
    "    list_result = []\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(\"merge_hyphenated\", before='parser')\n",
    "\n",
    "    for text in list_text:\n",
    "        list_word_df = process(nlp, text)\n",
    "        list_result.append(list_word_df)\n",
    "\n",
    "    df_result = pd.concat(list_result)\n",
    "\n",
    "    list_tmp = ['PUNCT', 'SPACE', 'AUX', 'CCONJ', 'SCONJ', 'ADP', 'PART', 'DET']\n",
    "    df_result.loc[df_result['part_of_speech'].isin(list_tmp), 'word_count'] = 0\n",
    "\n",
    "    list_tmp = ['CARDINAL', 'MONEY', 'PERCENT', 'DATE']\n",
    "    df_result.loc[df_result['category'].isin(list_tmp), 'word_count'] = 0\n",
    "\n",
    "    list_tmp = ['*', '=', '-', '+', '/', 'NR']\n",
    "    df_result.loc[df_result['word'].isin(list_tmp), 'word_count'] = 0\n",
    "\n",
    "    df_result.loc[df_result['word'].apply(len)==1, 'word_count'] = 0\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word = step1([original_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP2_PROMPT = \"\"\"The following meta table contains metadata extracted from Text according to the definition. Fill in the values for \"Part of Speech\", \"Category\", and \"Meaning\" in the table, and output the JSON in the format {{'data' : [ {{'word': str, 'part_of_speech': str, 'category': str, 'meaning': str}}, ...]}}.\n",
    "\n",
    "## Meta Table\n",
    "{step1_output}\n",
    "\n",
    "## Definition\n",
    "- Category: One of \"organization name, individual name, technical term\"\n",
    "- Meaning: Words that express higher-level concepts (multiple possible). Do not use other \"words\".\n",
    "\n",
    "## Example\n",
    "Word | Part of Speech | Category | Meaning\n",
    "---|---|---|---\n",
    "Medical Team | Common Noun | Organization Name | Healthcare\n",
    "Relay Station | Common Noun | Technical Term | Communication\n",
    "Tanaka Vehicles | Proper Noun | Organization Name | Company, Manufacturing\n",
    "Chronowar | Proper Noun | Technical Term | Product Name\n",
    "Napoleon | Proper Noun | Individual Name | Historical Figure\n",
    "\n",
    "## Text\n",
    "{context_input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_markdown(df:pd.DataFrame):\n",
    "    meta_table_s2 = ' | '.join(df.columns) + '\\n'\n",
    "    meta_table_s2 += ' | '.join(['---']*len(df.columns.to_list())) + \"\\n\"\n",
    "    meta_table_s2 += '\\n'.join(df.apply(lambda x: ' | '.join([x[col] for col in df.columns]), axis=1).values)\n",
    "\n",
    "    return meta_table_s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step2(client:OpenAI, model:str, df_word:pd.DataFrame, text:str) -> pd.DataFrame:\n",
    "    \"\"\"Create a word list at the lemma level from df_word and add category and meaning for content words\"\"\"\n",
    "    list_result = []\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(\"merge_hyphenated\", before='parser')\n",
    "\n",
    "    # Group by lemma\n",
    "    cols_unique = ['part_of_speech', 'category']\n",
    "    df_gr_lemma = df_word.groupby('lemma').agg({col:'unique' for col in cols_unique}).reset_index().copy()\n",
    "    for col in cols_unique:\n",
    "        df_gr_lemma[col] = df_gr_lemma[col].apply(', '.join)\n",
    "    df_gr_lemma = df_gr_lemma.rename(columns={'lemma':'word'})\n",
    "    df_gr_lemma['meaning'] = \"\"\n",
    "    df_gr_lemma['word_lower'] = df_gr_lemma['word'].apply(lambda x: x.lower().strip(string.punctuation))# 紐づけ用\n",
    "\n",
    "    # Process 10 words at a time to avoid failure\n",
    "    for start in range(0, len(df_gr_lemma), 10):\n",
    "        end = start + 10\n",
    "        chunk = df_gr_lemma.iloc[start:end]\n",
    "        \n",
    "        # Reprocess until the meaning is filled\n",
    "        total_output_count = chunk.shape[0]\n",
    "        list_tmp_result = []\n",
    "        chunk_count = 10 \n",
    "        while (total_output_count > 0) and (chunk_count > 0) :\n",
    "            print(total_output_count, chunk['word'].unique())\n",
    "            md_metatable = convert_df_to_markdown(chunk.drop(columns='word_lower'))\n",
    "\n",
    "            messages = []\n",
    "            prompt = STEP2_PROMPT.format(\n",
    "                step1_output=md_metatable, \n",
    "                context_input=text\n",
    "            )\n",
    "            messages = append_message(USER, prompt, messages)\n",
    "            \n",
    "            # Generate\n",
    "            count = trial_n\n",
    "            chunk_count -= 1\n",
    "            while count > 0:\n",
    "                try:\n",
    "                    response = generate(client, model, messages, stream=False, output_json=True).choices[0].message.content\n",
    "                    data_s2 = json.loads(response)['data']\n",
    "                    df_tmp = pd.DataFrame(data_s2)\n",
    "\n",
    "                    # 1. Convert to lowercase and remove punctuation and join them\n",
    "                    df_tmp_1 = df_tmp.copy()\n",
    "                    df_tmp_1['word_lower'] = df_tmp_1['word'].apply(lambda x: x.lower().strip(string.punctuation)) \n",
    "                    df_tmp_1 = df_tmp_1[df_tmp_1['word_lower'].isin(chunk['word_lower'].unique())].copy()\n",
    "                    df_tmp_1 = df_tmp_1[(df_tmp_1['meaning'].notna())&(df_tmp_1['meaning']!='')]\n",
    "                    print(df_tmp_1['word'].unique())\n",
    "                    list_tmp_result.append(df_tmp_1.drop(columns='word_lower').copy())\n",
    "                    chunk = chunk[~chunk['word_lower'].isin(df_tmp_1['word_lower'].unique())]\n",
    "\n",
    "                    # 2. Convert the extracted words to lemmas and join them\n",
    "                    if chunk.shape[0] > 0:\n",
    "                        df_tmp_2 = df_tmp.loc[~df_tmp.index.isin(df_tmp_1.index.to_list())].copy()\n",
    "                        df_tmp_2['lemma_'] = df_tmp_2['word']\n",
    "                        for i, row in df_tmp_2.iterrows():\n",
    "                            doc = nlp(row['word'])\n",
    "                            df_tmp.loc[i, 'lemma_'] = doc[0:].lemma_\n",
    "\n",
    "                        # Join\n",
    "                        df_tmp_2 = df_tmp_2[df_tmp_2['lemma_'].isin(chunk['word'])]\n",
    "                        print(df_tmp_2['lemma_'].unique())\n",
    "                        list_tmp_result.append(df_tmp_2.drop(columns='lemma_').copy())\n",
    "                        chunk = chunk[~chunk['word'].isin(df_tmp_2['lemma_'].unique())]\n",
    "\n",
    "                    total_output_count = chunk.shape[0]\n",
    "                    break\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(': error', e)\n",
    "                    count -= 1\n",
    "            \n",
    "        list_result.extend(list_tmp_result)\n",
    "\n",
    "    return pd.concat(list_result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemma = step2(ollama_client, ollama_model, df_word[df_word['word_count']!=0], original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word.to_csv(output_path_step1, encoding='utf-8-sig', index=False)\n",
    "df_lemma.to_csv(output_path_step2, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a word list for code conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_use = df_word[df_word['word_count']!=0].copy()\n",
    "df_word_use['code'] = df_word_use.groupby('lemma').ngroup()\n",
    "df_word_use['code'] = df_word_use['code'].rank(method='dense').astype(int)\n",
    "df_word_use['code'] = df_word_use['code'].apply(lambda x: \"r\"+str(x).zfill(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging process\n",
    "list_merge = []\n",
    "checked_index = []\n",
    "usecols = ['word', 'part_of_speech', 'category', 'lemma', 'word_count', 'index', 'code',\n",
    "       'part_of_speech_output', 'category_output', 'meaning']\n",
    "# 1. Merge the lemma and word columns\n",
    "df_merge_tmp = df_word_use.merge(df_lemma.reset_index(), how='inner', left_on=['lemma'], right_on=['word'], suffixes=['', '_output'])\n",
    "list_merge.append(df_merge_tmp[usecols].fillna('').copy())\n",
    "df_lemma_tmp = df_lemma.loc[~df_lemma.index.isin(df_merge_tmp['index_output'])].copy()[df_lemma.columns]\n",
    "checked_index.extend(df_merge_tmp.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  2. Convert all to lowercase and merge\n",
    "df_word_use_tmp = df_word_use[~df_word_use.index.isin(checked_index)].copy()\n",
    "df_word_use_tmp['word_lower'] = df_word_use_tmp['lemma'].apply(lambda x: x.lower().strip(string.punctuation))\n",
    "df_lemma_tmp['word_lower'] = df_lemma_tmp['word'].apply(lambda x: x.lower().strip(string.punctuation))\n",
    "df_merge_tmp = df_word_use_tmp.merge(df_lemma_tmp.drop(columns=['word']).reset_index(), how='inner', on='word_lower', suffixes=['', '_output'])\n",
    "list_merge.append(df_merge_tmp[usecols].fillna('').copy())\n",
    "df_lemma_tmp = df_lemma_tmp.loc[~df_lemma_tmp.index.isin(df_merge_tmp['index_output'])].copy()[df_lemma.columns]\n",
    "checked_index.extend(df_merge_tmp.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Reconvert the word in df_lemma to a lemma and merge\n",
    "df_word_use_tmp = df_word_use[~df_word_use.index.isin(checked_index)].copy()\n",
    "df_word_use_tmp['word_lower'] = df_word_use_tmp['lemma'].apply(lambda x: x.lower().strip(string.punctuation))\n",
    "df_lemma_tmp['lemma'] = None\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"merge_hyphenated\", before='parser')\n",
    "\n",
    "for i, row in df_lemma_tmp.iterrows():\n",
    "    doc = nlp(row['word'])\n",
    "    df_lemma_tmp.loc[i, 'lemma'] = doc[0:].lemma_\n",
    "\n",
    "df_lemma_tmp['word_lower'] = df_lemma_tmp['lemma'].apply(lambda x: x.lower().strip(string.punctuation))\n",
    "\n",
    "df_merge_tmp = df_word_use_tmp.merge(df_lemma_tmp.drop(columns=['word']).reset_index(), how='inner', on='word_lower', suffixes=['', '_output'])\n",
    "list_merge.append(df_merge_tmp[usecols].fillna('').copy())\n",
    "df_lemma_tmp = df_lemma_tmp.loc[~df_lemma_tmp.index.isin(df_merge_tmp['index_output'])].copy()[df_lemma.columns]\n",
    "checked_index.extend(df_merge_tmp.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_meaning = pd.concat(list_merge + [df_word_use.loc[~df_word_use.index.isin(checked_index)]], ignore_index=True)\n",
    "\n",
    "for col in ['part_of_speech', 'category']:\n",
    "    df_word_meaning[f\"{col}_new\"] = df_word_meaning[f'{col}']\n",
    "    df_word_meaning.loc[df_word_meaning[f\"{col}_new\"]==\"\", f\"{col}_new\"] = df_word_meaning[df_word_meaning[f\"{col}_new\"]==\"\"][f'{col}_output']\n",
    "\n",
    "df_word_new = df_word_meaning[['lemma', 'word', 'part_of_speech_new', 'category_new', 'meaning', 'code']].rename(columns=\n",
    "    {\n",
    "        'part_of_speech_new': 'part_of_speech',    \n",
    "        'category_new': 'category',    \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_new.to_csv(output_path_step2_use, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step4\n",
    "- Select words to be coded based on the masking rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 12345\n",
    "\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_mask_row(df_word:pd.DataFrame, mask_rate:int):\n",
    "    \"\"\"Select words to be coded based on the masking rate\"\"\"\n",
    "    assert (mask_rate >= 0) and (mask_rate <= 100), f\"`mask_rate` must be set between 0 and 100. mask_rate: {mask_rate}\"\n",
    "\n",
    "    df_copy = df_word.copy()\n",
    "    mask_col = f'p_{mask_rate}_masked'\n",
    "    df_copy[mask_col] = False\n",
    "\n",
    "    list_lemma = df_word['lemma'].unique().tolist()\n",
    "    mask_row_n = round(len(list_lemma) * (mask_rate/100))\n",
    "    print(f'{mask_rate}% number of words to be masked:', mask_row_n)\n",
    "\n",
    "    list_mask_lemma = random.sample(list_lemma, k=mask_row_n)\n",
    "    mask_rows_index = df_word[df_word['lemma'].isin(list_mask_lemma)].index.values\n",
    "\n",
    "    df_copy.loc[mask_rows_index, mask_col] = True\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mask_rate in list_mask_rate:\n",
    "    df_word_new = select_mask_row(df_word_new, mask_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step4(text:str, list_mask_rate:list[int], df_word_new:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return masked text based on the masking rate\"\"\"\n",
    "    df_copy = pd.DataFrame()\n",
    "    for mask_rate in list_mask_rate:\n",
    "        df_copy[f\"s4_prg_encode_{mask_rate}\"] = \"\"\n",
    "\n",
    "        values = df_word_new[df_word_new[f'p_{mask_rate}_masked']].apply(lambda x: {x['code']:x['word']}, axis=1)\n",
    "        values = sorted(values, key=lambda x: x[next(iter(x))].count(' '), reverse=True)\n",
    "\n",
    "        sub_text = text\n",
    "        for pair in values:\n",
    "            code = list(pair.keys())[0]\n",
    "            word = list(pair.values())[0]\n",
    "            sub_text = sub_text.replace(word, f\"<{code}>\")\n",
    "        df_copy.loc[0, f\"s4_prg_encode_{mask_rate}\"] = sub_text\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa = step4(original_text, list_mask_rate, df_word_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa.to_csv(output_path_qa_tmp, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_new.to_csv(output_path_step4, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_new = pd.read_csv(output_path_step4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the missing rate of meanings for each masking rate\n",
    "list_tmp = []\n",
    "for mask_rate in list_mask_rate:\n",
    "    if mask_rate == 0:\n",
    "        continue\n",
    "    \n",
    "    tmp_total_lemma = df_word_new[df_word_new[f'p_{mask_rate}_masked']]['lemma'].nunique()\n",
    "    tmp_empty_meaning_lemma = tmp_total_lemma - df_word_new[df_word_new[f'p_{mask_rate}_masked'] & df_word_new['meaning'].notna()]['lemma'].nunique()\n",
    "    list_tmp.append({'MR': mask_rate, 'number of words with missing meanings': tmp_empty_meaning_lemma, 'word count': tmp_total_lemma, 'missing rate of meanings': tmp_empty_meaning_lemma / tmp_total_lemma})\n",
    "    \n",
    "pd.DataFrame(list_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step5\n",
    "- generate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP5_PROMPT = \"\"\"{text}\n",
    "Therefore, the total recall cost X is, X = 8,000 * NR =\n",
    "Since the planned sales volume is B + C, considering the reduction rate, the post-recall sales volume N is, N = (B + C) * (1 - 0.25) =\n",
    "The decrease in revenue Y is, Y = P * (B + C) * 0.25 =\n",
    "The loss amount L is, L = X + Y =\n",
    "The revised planned sales revenue for the model subject to recall, E', is, E' = E - L =\n",
    "The revised planned revenue for this fiscal year, D', is, D' = D - L =\n",
    "\n",
    "<Meta Information>\n",
    "{metadata}\n",
    "</Meta Information>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def exec_step5(client:AsyncOpenAI, model:str, i:int, output_col:str, query_col:str, text:str, metadata: str, count:int):\n",
    "    messages = []\n",
    "    prompt = STEP5_PROMPT.format(\n",
    "        text=text,\n",
    "        metadata=metadata\n",
    "    )\n",
    "    messages = append_message(USER, prompt, messages)\n",
    "\n",
    "    while count > 0:\n",
    "        try:\n",
    "            res = await aget_response(client, model, messages)\n",
    "            ret = (i, res, prompt, output_col, query_col)\n",
    "            return ret\n",
    "\n",
    "        except Exception as e:\n",
    "            print('error', e)\n",
    "            count -= 1\n",
    "\n",
    "    return (i, '-1', '', output_col, query_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def step5(aclient:AsyncOpenAI, model:str, df_qa:pd.DataFrame, df_word_new:pd.DataFrame) -> pd.DataFrame:\n",
    "    df_copy = df_qa.copy()\n",
    "    print('model:', model)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for mask_rate in list_mask_rate:\n",
    "        tasks = []\n",
    "        \n",
    "        output_col = f'answer_{model}_{mask_rate}'\n",
    "        query_col = f'query_{mask_rate}'\n",
    "        df_copy[output_col] = \"\"\n",
    "        df_copy[query_col] = \"\"\n",
    "        \n",
    "        col_q = f's4_prg_encode_{mask_rate}'\n",
    "\n",
    "        for i, row in df_copy.iterrows():\n",
    "            meta_table = df_word_new[df_word_new[f'p_{mask_rate}_masked']].groupby('lemma').agg(\n",
    "                {col: 'unique' for col in ['part_of_speech', 'category', 'meaning', 'code']}\n",
    "            ).copy()\n",
    "            if not meta_table.empty:\n",
    "                for col in ['part_of_speech', 'category', 'meaning', 'code']:\n",
    "                    meta_table[col] = meta_table[col].apply(lambda x: ', '.join(t for t in x if type(t)==str))\n",
    "            md_metatable = convert_df_to_markdown(meta_table)\n",
    "            count = trial_n\n",
    "            tasks.append(asyncio.ensure_future(exec_step5(aclient, model, i, output_col, query_col, row[col_q], md_metatable, count)))\n",
    "\n",
    "        results.extend(await asyncio.gather(*tasks))\n",
    "\n",
    "    for i, ans, query, output_col, query_col in results:\n",
    "        df_copy.loc[i, output_col] = ans\n",
    "        df_copy.loc[i, query_col] = query\n",
    "\n",
    "    return df_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output_num in range(10):\n",
    "    print('trial:', output_num) \n",
    "    df_result = await step5(openai_async_client, openai_model, df_qa, df_word_new)\n",
    "    save_path_tmp = str(output_path_qa).format(trial_no=output_num)\n",
    "    print(save_path_tmp)\n",
    "    df_result.to_csv(save_path_tmp, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = \"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_qa = dir_output / f\"{QA_TYPE}_{openai_model.replace(':', '_')}_{{trial_no}}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa = pd.read_csv(output_path_qa_tmp, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output_num in range(10):\n",
    "    print('trial:', output_num) \n",
    "    df_result = await step5(openai_async_client, openai_model, df_qa, df_word_new)\n",
    "    save_path_tmp = str(output_path_qa).format(trial_no=output_num)\n",
    "    print(save_path_tmp)\n",
    "    df_result.to_csv(save_path_tmp, encoding='utf-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
